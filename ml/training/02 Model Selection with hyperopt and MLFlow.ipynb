{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2be4ad19-f1da-4b16-8abe-818f39bd3696",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK,Trials\n",
    "import mlflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b404a4a-ee89-470e-bcf0-9f830ce492e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X,y = fetch_california_housing(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2c7ecd8-3e80-442b-863c-ef10931b02d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   8.3252       41.            6.98412698 ...    2.55555556\n    37.88       -122.23      ]\n [   8.3014       21.            6.23813708 ...    2.10984183\n    37.86       -122.22      ]\n [   7.2574       52.            8.28813559 ...    2.80225989\n    37.85       -122.24      ]\n ...\n [   1.7          17.            5.20554273 ...    2.3256351\n    39.43       -121.22      ]\n [   1.8672       18.            5.32951289 ...    2.12320917\n    39.43       -121.32      ]\n [   2.3886       16.            5.25471698 ...    2.61698113\n    39.37       -121.24      ]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "957a06e0-53af-41cf-82e1-86e3f3f64074",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.526 3.585 3.521 ... 0.923 0.847 0.894]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72da99e2-6a26-478d-9e73-1455b2f1bc52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 3.87067100e+00,  2.86394864e+01,  5.42899974e+00,  1.09667515e+00,\n",
       "        1.42547674e+03,  3.07065516e+00,  3.56318614e+01, -1.19569704e+02])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb9286d9-1cc6-4c7e-947d-3a9d7c8b2317",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3953fc78-0fa4-4ede-90a5-71f9210515a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5647243-2779-4953-a25a-4d1e715ad6a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 6.60969987e-17,  5.50808322e-18,  6.60969987e-17, -1.06030602e-16,\n",
       "       -1.10161664e-17,  3.44255201e-18, -1.07958431e-15, -8.52651283e-15])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "366742c2-60ec-4bd9-9ca8-7338f90eecb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert the numeric target value to discrete values. We want to do classification so we move from continious values to discreet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcbb85ad-e04c-49fc-b474-c28956891ac8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "y_discrete = np.where(y < np.median(y), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00992c12-3501-4cc8-8e3d-d10aa4e720f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define the objective function\n",
    "def objective(params):\n",
    "    classifier_type = params['type']\n",
    "    del params['type']\n",
    "    if classifier_type == 'svm':\n",
    "        clf = SVC(**params)\n",
    "    elif classifier_type == 'rf':\n",
    "        clf = RandomForestClassifier(**params)\n",
    "    elif classifier_type == 'logreg':\n",
    "        clf = LogisticRegression(**params)\n",
    "    else:\n",
    "        return 0\n",
    "    accuracy = cross_val_score(clf,X,y_discrete,cv=5).mean()    \n",
    "\n",
    "    return {\"loss\":-accuracy,\"status\":STATUS_OK}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e6d3746-8172-4e2f-9981-112152bbafbd",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Lets create the search space\n",
    "search_space = hp.choice('classifier_type',[\n",
    "                             {\n",
    "                                 'type':'svm',\n",
    "                                 'C':hp.lognormal('SVM_C',0,1.0),\n",
    "                                 'kernel':hp.choice('kernel',['linear','rbf'])\n",
    "                             },\n",
    "                             {\n",
    "                                 'type':'rf',\n",
    "                                 'max_depth':hp.choice('max_depth', np.arange(2, 5, dtype=int)),\n",
    "                                 'criterion':hp.choice('criterion',['gini','entropy'])\n",
    "                             },\n",
    "                             {\n",
    "                                 'type':'logreg',\n",
    "                                 'C':hp.lognormal('LR_C',0,1.0),\n",
    "                                 'solver':hp.choice('sovler',['liblinear','lbfgs'])\n",
    "                             }\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfc77090-5ea2-4c5c-be6e-ed69319e80bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select the search algorithm\n",
    "algo = tpe.suggest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55c5fecb-94b5-4e4c-8ec2-2a91da3b9e87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Because the requested parallelism was None or a non-positive value, parallelism will be set to (4), which is Spark's default parallelism (4), or 1, whichever is greater. We recommend setting parallelism explicitly to a positive value because the total of Spark task slots is subject to cluster sizing.\n"
     ]
    }
   ],
   "source": [
    "# Use distributed tuning\n",
    "spark_trials = SparkTrials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8333091e-1034-45ef-a66d-b3cc1d621701",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperopt with SparkTrials will automatically track trials in MLflow. To view the MLflow experiment associated with the notebook, click the 'Runs' icon in the notebook context bar on the upper right. There, you can view all runs.\nTo view logs from trials, please check the Spark executor logs. To view executor logs, expand 'Spark Jobs' above until you see the (i) icon next to the stage from the trial job. Click it and find the list of tasks. Click the 'stderr' link for a task to view trial logs.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/10 [00:00<?, ?trial/s, best loss=?]\r 10%|█         | 1/10 [00:07<01:05,  7.23s/trial, best loss: -0.814922480620155]\r 30%|███       | 3/10 [00:11<00:23,  3.36s/trial, best loss: -0.814922480620155]\r 40%|████      | 4/10 [00:16<00:23,  3.93s/trial, best loss: -0.814922480620155]\r 50%|█████     | 5/10 [00:22<00:23,  4.62s/trial, best loss: -0.814922480620155]\r 60%|██████    | 6/10 [00:32<00:25,  6.35s/trial, best loss: -0.814922480620155]\r 70%|███████   | 7/10 [00:42<00:22,  7.51s/trial, best loss: -0.814922480620155]\r 80%|████████  | 8/10 [00:52<00:16,  8.35s/trial, best loss: -0.8217538759689923]\r 90%|█████████ | 9/10 [00:56<00:07,  7.07s/trial, best loss: -0.8242732558139535]\r100%|██████████| 10/10 [01:01<00:00,  6.22s/trial, best loss: -0.8243217054263565]\r100%|██████████| 10/10 [01:01<00:00,  6.10s/trial, best loss: -0.8243217054263565]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Trials: 10: 10 succeeded, 0 failed, 0 cancelled.\n"
     ]
    }
   ],
   "source": [
    "# using mflow.start_run(), the hyperopt parameters are automaically tracked\n",
    "with mlflow.start_run():\n",
    "    best = fmin(fn=objective, space=search_space, algo=algo, max_evals=10, trials=spark_trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15ed7145-d72e-468e-9699-37ec52f587f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.3628851580609844, 'kernel': 'linear', 'type': 'svm'}\n"
     ]
    }
   ],
   "source": [
    "# Print the hyperparameters\n",
    "import hyperopt\n",
    "print(hyperopt.space_eval(search_space,best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63d4910d-d559-4a2d-89c8-96e8aebc4d07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02 Model Selection with hyperopt and MLFlow",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
