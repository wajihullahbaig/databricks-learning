{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac9e691a-efeb-4548-b753-9f5f627480e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Multi machine hyperopt\n",
    "Here are the steps for the hyperopt flow\n",
    "- Define a function to minimize using spark MLlib\n",
    "- Define a search space over hyper parameters\n",
    "- Select the search algorithm\n",
    "- Run the tuning algorithm with Hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1beab09b-f9fe-4a01-bd04-f9c00ddab1b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will be using hyperopt and sparks mllib to perform a distributed tuning of hyperparameters while training a ml algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d4730c5-6175-4ab9-ad84-8df199abbb41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/COVID/</td><td>COVID/</td><td>0</td><td>1727855155408</td></tr><tr><td>dbfs:/databricks-datasets/README.md</td><td>README.md</td><td>976</td><td>1532502332000</td></tr><tr><td>dbfs:/databricks-datasets/Rdatasets/</td><td>Rdatasets/</td><td>0</td><td>1727855155408</td></tr><tr><td>dbfs:/databricks-datasets/SPARK_README.md</td><td>SPARK_README.md</td><td>3359</td><td>1455505270000</td></tr><tr><td>dbfs:/databricks-datasets/adult/</td><td>adult/</td><td>0</td><td>1727855155408</td></tr><tr><td>dbfs:/databricks-datasets/airlines/</td><td>airlines/</td><td>0</td><td>1727855155408</td></tr><tr><td>dbfs:/databricks-datasets/amazon/</td><td>amazon/</td><td>0</td><td>1727855155408</td></tr><tr><td>dbfs:/databricks-datasets/asa/</td><td>asa/</td><td>0</td><td>1727855155408</td></tr><tr><td>dbfs:/databricks-datasets/atlas_higgs/</td><td>atlas_higgs/</td><td>0</td><td>1727855155408</td></tr><tr><td>dbfs:/databricks-datasets/bikeSharing/</td><td>bikeSharing/</td><td>0</td><td>1727855155408</td></tr><tr><td>dbfs:/databricks-datasets/cctvVideos/</td><td>cctvVideos/</td><td>0</td><td>1727855155408</td></tr><tr><td>dbfs:/databricks-datasets/credit-card-fraud/</td><td>credit-card-fraud/</td><td>0</td><td>1727855155408</td></tr><tr><td>dbfs:/databricks-datasets/cs100/</td><td>cs100/</td><td>0</td><td>1727855155408</td></tr><tr><td>dbfs:/databricks-datasets/cs110x/</td><td>cs110x/</td><td>0</td><td>1727855155408</td></tr><tr><td>dbfs:/databricks-datasets/cs190/</td><td>cs190/</td><td>0</td><td>1727855155408</td></tr><tr><td>dbfs:/databricks-datasets/data.gov/</td><td>data.gov/</td><td>0</td><td>1727855155408</td></tr><tr><td>dbfs:/databricks-datasets/definitive-guide/</td><td>definitive-guide/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/delta-sharing/</td><td>delta-sharing/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/flights/</td><td>flights/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/flower_photos/</td><td>flower_photos/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/flowers/</td><td>flowers/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/genomics/</td><td>genomics/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/hail/</td><td>hail/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/identifying-campaign-effectiveness/</td><td>identifying-campaign-effectiveness/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/iot/</td><td>iot/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/iot-stream/</td><td>iot-stream/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark/</td><td>learning-spark/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark-v2/</td><td>learning-spark-v2/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/lending-club-loan-stats/</td><td>lending-club-loan-stats/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/med-images/</td><td>med-images/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/media/</td><td>media/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/mnist-digits/</td><td>mnist-digits/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/news20.binary/</td><td>news20.binary/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/nyctaxi/</td><td>nyctaxi/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/nyctaxi-with-zipcodes/</td><td>nyctaxi-with-zipcodes/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/online_retail/</td><td>online_retail/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/overlap-join/</td><td>overlap-join/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/power-plant/</td><td>power-plant/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/retail-org/</td><td>retail-org/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/rwe/</td><td>rwe/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/sai-summit-2019-sf/</td><td>sai-summit-2019-sf/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/sample_logs/</td><td>sample_logs/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/samples/</td><td>samples/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/sfo_customer_survey/</td><td>sfo_customer_survey/</td><td>0</td><td>1727855155409</td></tr><tr><td>dbfs:/databricks-datasets/sms_spam_collection/</td><td>sms_spam_collection/</td><td>0</td><td>1727855155410</td></tr><tr><td>dbfs:/databricks-datasets/songs/</td><td>songs/</td><td>0</td><td>1727855155410</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/</td><td>structured-streaming/</td><td>0</td><td>1727855155410</td></tr><tr><td>dbfs:/databricks-datasets/timeseries/</td><td>timeseries/</td><td>0</td><td>1727855155410</td></tr><tr><td>dbfs:/databricks-datasets/tpch/</td><td>tpch/</td><td>0</td><td>1727855155410</td></tr><tr><td>dbfs:/databricks-datasets/travel_recommendations_realtime/</td><td>travel_recommendations_realtime/</td><td>0</td><td>1727855155410</td></tr><tr><td>dbfs:/databricks-datasets/warmup/</td><td>warmup/</td><td>0</td><td>1727855155410</td></tr><tr><td>dbfs:/databricks-datasets/weather/</td><td>weather/</td><td>0</td><td>1727855155410</td></tr><tr><td>dbfs:/databricks-datasets/wiki/</td><td>wiki/</td><td>0</td><td>1727855155410</td></tr><tr><td>dbfs:/databricks-datasets/wikipedia-datasets/</td><td>wikipedia-datasets/</td><td>0</td><td>1727855155410</td></tr><tr><td>dbfs:/databricks-datasets/wine-quality/</td><td>wine-quality/</td><td>0</td><td>1727855155410</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/databricks-datasets/COVID/",
         "COVID/",
         0,
         1727855155408
        ],
        [
         "dbfs:/databricks-datasets/README.md",
         "README.md",
         976,
         1532502332000
        ],
        [
         "dbfs:/databricks-datasets/Rdatasets/",
         "Rdatasets/",
         0,
         1727855155408
        ],
        [
         "dbfs:/databricks-datasets/SPARK_README.md",
         "SPARK_README.md",
         3359,
         1455505270000
        ],
        [
         "dbfs:/databricks-datasets/adult/",
         "adult/",
         0,
         1727855155408
        ],
        [
         "dbfs:/databricks-datasets/airlines/",
         "airlines/",
         0,
         1727855155408
        ],
        [
         "dbfs:/databricks-datasets/amazon/",
         "amazon/",
         0,
         1727855155408
        ],
        [
         "dbfs:/databricks-datasets/asa/",
         "asa/",
         0,
         1727855155408
        ],
        [
         "dbfs:/databricks-datasets/atlas_higgs/",
         "atlas_higgs/",
         0,
         1727855155408
        ],
        [
         "dbfs:/databricks-datasets/bikeSharing/",
         "bikeSharing/",
         0,
         1727855155408
        ],
        [
         "dbfs:/databricks-datasets/cctvVideos/",
         "cctvVideos/",
         0,
         1727855155408
        ],
        [
         "dbfs:/databricks-datasets/credit-card-fraud/",
         "credit-card-fraud/",
         0,
         1727855155408
        ],
        [
         "dbfs:/databricks-datasets/cs100/",
         "cs100/",
         0,
         1727855155408
        ],
        [
         "dbfs:/databricks-datasets/cs110x/",
         "cs110x/",
         0,
         1727855155408
        ],
        [
         "dbfs:/databricks-datasets/cs190/",
         "cs190/",
         0,
         1727855155408
        ],
        [
         "dbfs:/databricks-datasets/data.gov/",
         "data.gov/",
         0,
         1727855155408
        ],
        [
         "dbfs:/databricks-datasets/definitive-guide/",
         "definitive-guide/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/delta-sharing/",
         "delta-sharing/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/flights/",
         "flights/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/flower_photos/",
         "flower_photos/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/flowers/",
         "flowers/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/genomics/",
         "genomics/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/hail/",
         "hail/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/identifying-campaign-effectiveness/",
         "identifying-campaign-effectiveness/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/iot/",
         "iot/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/iot-stream/",
         "iot-stream/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/learning-spark/",
         "learning-spark/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/learning-spark-v2/",
         "learning-spark-v2/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/lending-club-loan-stats/",
         "lending-club-loan-stats/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/med-images/",
         "med-images/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/media/",
         "media/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/mnist-digits/",
         "mnist-digits/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/news20.binary/",
         "news20.binary/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/nyctaxi/",
         "nyctaxi/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/nyctaxi-with-zipcodes/",
         "nyctaxi-with-zipcodes/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/online_retail/",
         "online_retail/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/overlap-join/",
         "overlap-join/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/power-plant/",
         "power-plant/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/retail-org/",
         "retail-org/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/rwe/",
         "rwe/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/sai-summit-2019-sf/",
         "sai-summit-2019-sf/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/sample_logs/",
         "sample_logs/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/samples/",
         "samples/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/sfo_customer_survey/",
         "sfo_customer_survey/",
         0,
         1727855155409
        ],
        [
         "dbfs:/databricks-datasets/sms_spam_collection/",
         "sms_spam_collection/",
         0,
         1727855155410
        ],
        [
         "dbfs:/databricks-datasets/songs/",
         "songs/",
         0,
         1727855155410
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/",
         "structured-streaming/",
         0,
         1727855155410
        ],
        [
         "dbfs:/databricks-datasets/timeseries/",
         "timeseries/",
         0,
         1727855155410
        ],
        [
         "dbfs:/databricks-datasets/tpch/",
         "tpch/",
         0,
         1727855155410
        ],
        [
         "dbfs:/databricks-datasets/travel_recommendations_realtime/",
         "travel_recommendations_realtime/",
         0,
         1727855155410
        ],
        [
         "dbfs:/databricks-datasets/warmup/",
         "warmup/",
         0,
         1727855155410
        ],
        [
         "dbfs:/databricks-datasets/weather/",
         "weather/",
         0,
         1727855155410
        ],
        [
         "dbfs:/databricks-datasets/wiki/",
         "wiki/",
         0,
         1727855155410
        ],
        [
         "dbfs:/databricks-datasets/wikipedia-datasets/",
         "wikipedia-datasets/",
         0,
         1727855155410
        ],
        [
         "dbfs:/databricks-datasets/wine-quality/",
         "wine-quality/",
         0,
         1727855155410
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls('/databricks-datasets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b628bb58-a9b6-4a4a-a9a3-f578284860ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/mnist-digits/README.md</td><td>README.md</td><td>640</td><td>1455505289000</td></tr><tr><td>dbfs:/databricks-datasets/mnist-digits/data-001/</td><td>data-001/</td><td>0</td><td>1727855166376</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/databricks-datasets/mnist-digits/README.md",
         "README.md",
         640,
         1455505289000
        ],
        [
         "dbfs:/databricks-datasets/mnist-digits/data-001/",
         "data-001/",
         0,
         1727855166376
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls('/databricks-datasets/mnist-digits'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a382ecef-d271-4375-af0a-30e2c04a808a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/mnist-digits/data-001/mnist-digits-test.txt</td><td>mnist-digits-test.txt</td><td>11671108</td><td>1455505289000</td></tr><tr><td>dbfs:/databricks-datasets/mnist-digits/data-001/mnist-digits-train.txt</td><td>mnist-digits-train.txt</td><td>69430283</td><td>1455505289000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/databricks-datasets/mnist-digits/data-001/mnist-digits-test.txt",
         "mnist-digits-test.txt",
         11671108,
         1455505289000
        ],
        [
         "dbfs:/databricks-datasets/mnist-digits/data-001/mnist-digits-train.txt",
         "mnist-digits-train.txt",
         69430283,
         1455505289000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls('/databricks-datasets/mnist-digits/data-001'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b87befa7-3325-4a22-8360-a8607532f509",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre>======================================\n",
       "MNIST handwritten digits dataset\n",
       "======================================\n",
       "\n",
       "## Data Source\n",
       "LibSVM Datasets\n",
       "https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#mnist\n",
       "Chih-Chung Chang and Chih-Jen Lin, LIBSVM : a library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1--27:27, 2011.\n",
       "\n",
       "## Original Data Set Source\n",
       "Yann LeCun, L. Bottou, Y. Bengio, and P. Haffner. \n",
       "Gradient-based learning applied to document recognition. \n",
       "Proceedings of the IEEE, 86(11):2278-2324, November 1998. \n",
       "MNIST database available at http://yann.lecun.com/exdb/mnist/\n",
       "</pre>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read the README.md file\n",
    "readme_path = 'dbfs:/databricks-datasets/mnist-digits/README.md'\n",
    "readme_content = dbutils.fs.head(readme_path)\n",
    "\n",
    "# Display the content as HTML\n",
    "displayHTML(f\"<pre>{readme_content}</pre>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64a462e2-7ac5-477c-94ad-dd268aebe1ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 60000 training images and 60000 test images\n"
     ]
    }
   ],
   "source": [
    "# Run distributed training using MLlib\n",
    "full_training_data = spark.read.format(\"libsvm\").load('dbfs:/databricks-datasets/mnist-digits/data-001/mnist-digits-train.txt')\n",
    "test_data = spark.read.format(\"libsvm\").load('dbfs:/databricks-datasets/mnist-digits/data-001/mnist-digits-train.txt')\n",
    "\n",
    "full_training_data.cache()\n",
    "test_data.cache()\n",
    "\n",
    "print(f\"There are {full_training_data.count()} training images and {test_data.count()} test images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb89b08b-2162-4233-afd4-4f98737b0593",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lets random split the training_data for tuning\n",
    "training_data, validation_data = full_training_data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7f689a7-557e-49f5-be48-0730118fee48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28ec9dda-2932-4f5c-b79b-c3695a22b0ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import mlflow.pyspark.ml\n",
    "    mlflow.pyspark.ml.autolog()\n",
    "except:\n",
    "    print(f\"Your version of mlflow is {mlflow.__verion__}, try upgrading to use auto logging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0be3b1a5-fbbf-4fce-821a-3048a10ab176",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lets train a decision tree classifier\n",
    "def train_tree(minInstancesPerNode, maxBins):\n",
    "    with mlflow.start_run(nested=True):\n",
    "        indexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\")\n",
    "        dtc = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", minInstancesPerNode=minInstancesPerNode, maxBins=maxBins)\n",
    "        # Create the pipe\n",
    "        pipeline = Pipeline(stages=[indexer, dtc])\n",
    "        # run the pipe\n",
    "        model = pipeline.fit(training_data)\n",
    "\n",
    "        # define evaluation metric\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "        predictions = model.transform(test_data)\n",
    "        validation_metric = evaluator.evaluate(predictions)\n",
    "        mlflow.log_metric(\"val_f1_score\", validation_metric)\n",
    "\n",
    "        return model,validation_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cc515d5-99cc-4e3d-b214-765ad30e00bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/02 07:46:29 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n2024/10/02 07:46:45 WARNING mlflow.pyspark.ml: Model inputs contain unsupported Spark data types: [StructField('features', VectorUDT(), True)]. Model signature is not logged.\n2024/10/02 07:47:06 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n2024/10/02 07:47:59 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2023-07-14; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'databricks-feature-engineering'}\n2024/10/02 07:47:59 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/databricks/python/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\n2024/10/02 07:48:01 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained decision tree achieved and F1 score of 0.6722346573614189\n"
     ]
    }
   ],
   "source": [
    "init_model, valid_metric = train_tree(minInstancesPerNode=20, maxBins=2)\n",
    "print(\"Trained decision tree achieved and F1 score of\", valid_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c574041c-ce35-4a1f-9894-dac6fe8cbec7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Lets do hyperopt to get best models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "763d53d8-1336-4d08-b006-8dca3919d6e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK\n",
    "def train_tree_hyperopt(params):\n",
    "    minInstancesPerNode = int(params['minInstancesPerNode'])\n",
    "    maxBins = int(params['maxBins'])\n",
    "    model, validation_metric = train_tree(minInstancesPerNode, maxBins)\n",
    "    loss = -validation_metric\n",
    "    return {'loss': loss, 'status': STATUS_OK}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8710d144-8e4b-489b-8629-0198b0d5b2f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "space = {\n",
    "    'minInstancesPerNode': hp.uniform('minInstancesPerNode',2,5),\n",
    "    'maxBins': hp.uniform('maxBins',2,5)\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20b2dfab-1dcf-4136-bc9f-9795b402aeca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Running the tuning algorithm with hyperopt. We do not need to pass trials to fmin(), by default it will use the 'Trials' class to run the trials on driver nodes.\n",
    "# Donot use SparkTrials with MlLib class, the class is desgined to work with algorithms that are not them selves distributed. MLlib uses distributed computing already and is not compatible with spark Trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8a6db12-04e0-417e-9d96-134b47a26b0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/10 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/02 07:48:13 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n2024/10/02 07:48:39 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n\n2024/10/02 07:49:29 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2023-07-14; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'databricks-feature-engineering'}\n\n2024/10/02 07:49:30 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 10%|█         | 1/10 [01:25<12:45, 85.06s/trial, best loss: -0.6802141350126889]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/02 07:49:38 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n2024/10/02 07:50:03 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n\n2024/10/02 07:50:53 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2023-07-14; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'databricks-feature-engineering'}\n\n2024/10/02 07:50:55 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 20%|██        | 2/10 [02:50<11:19, 84.99s/trial, best loss: -0.6808065313193914]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/02 07:51:03 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n2024/10/02 07:51:28 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n\n2024/10/02 07:52:17 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2023-07-14; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'databricks-feature-engineering'}\n\n2024/10/02 07:52:19 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 30%|███       | 3/10 [04:13<09:50, 84.38s/trial, best loss: -0.6808065313193914]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/02 07:52:27 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n2024/10/02 07:52:53 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with mlflow.start_run(): # track progress with mlflow.start_run()\n",
    "    best_params = fmin(fn=train_tree_hyperopt, space=space, algo = tpe.suggest, max_evals=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb090e2f-10f4-4a90-a8e8-6a6036bd60d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Best value found for parameters:\",best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1af4047f-1340-4885-a6d5-0daa93d6d72c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Retrain the model on training dataset using best parameter values\n",
    "### Run was stopped as it was getting expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7682e698-3168-45dc-a1c8-9afadada9fde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_minInstancesPerNode = 36\n",
    "best_maxBins = 25\n",
    "\n",
    "final_model ,val_f1_score = train_tree(best_minInstancesPerNode,best_maxBins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2999231-ec3e-43db-9a1e-63eabd51eec1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Validation F1 score:\",val_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5e1f299-00e1-46c5-bef9-d37b7f842a00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets test the dataset to compare evaluation metrics for initial and best model\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "init_model_test_metric = evaluator.evaluate(init_model.transform(test_data))\n",
    "final_model_test_metric = evaluator.evaluate(final_model.transform(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5f36b70-ecf0-4c56-8c08-2b30cef6fa06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Test data metrics untuned model f1 score:{init_model_test_metric} vs tuned model f1 score: {final_model_test_metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff0b5461-4e9a-4ad3-b8a5-89c06fbe4398",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03 Tuning in distributed training using Spark MLlib",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
